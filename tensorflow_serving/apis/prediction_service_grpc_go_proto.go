// Code generated by protoc-gen-go. DO NOT EDIT.
// source: third_party/tensorflow_serving/apis/prediction_service.proto

package prediction_service_grpc

import (
	context "context"
	fmt "fmt"
	proto "google3/net/proto2/go/proto"
	grpc "google3/third_party/golang/grpc/grpc"
	classification_go_proto "google3/third_party/tensorflow_serving/apis/classification_go_proto"
	get_model_metadata_go_proto "google3/third_party/tensorflow_serving/apis/get_model_metadata_go_proto"
	inference_go_proto "google3/third_party/tensorflow_serving/apis/inference_go_proto"
	predict_go_proto "google3/third_party/tensorflow_serving/apis/predict_go_proto"
	_ "google3/third_party/tensorflow_serving/apis/prediction_service_go_proto"
	regression_go_proto "google3/third_party/tensorflow_serving/apis/regression_go_proto"
	math "math"
	reflect "reflect"
)

// Reference imports to suppress errors if they are not otherwise used.
var _ = proto.Marshal
var _ = fmt.Errorf
var _ = math.Inf
var _ = reflect.ValueOf
var _ = context.Background

// This is a compile-time assertion to ensure that this generated file
// is compatible with the proto package it is being compiled against.
// A compilation error at this line likely means your copy of the
// proto package needs to be updated.
const _ = proto.ProtoPackageIsVersion3 // please upgrade the proto package

// Reference imports to suppress errors if they are not otherwise used.
var _ context.Context
var _ grpc.ClientConn

// This is a compile-time assertion to ensure that this generated file
// is compatible with the grpc package it is being compiled against.
const _ = grpc.SupportPackageIsVersion4

// PredictionServiceClient is the client API for PredictionService service.
//
// For semantics around ctx use and closing/ending streaming RPCs, please refer to https://godoc.org/google.golang.org/grpc#ClientConn.NewStream.
type PredictionServiceClient interface {
	// Classify.
	Classify(ctx context.Context, in *classification_go_proto.ClassificationRequest, opts ...grpc.CallOption) (*classification_go_proto.ClassificationResponse, error)
	// Regress.
	Regress(ctx context.Context, in *regression_go_proto.RegressionRequest, opts ...grpc.CallOption) (*regression_go_proto.RegressionResponse, error)
	// Predict -- provides access to loaded TensorFlow model.
	Predict(ctx context.Context, in *predict_go_proto.PredictRequest, opts ...grpc.CallOption) (*predict_go_proto.PredictResponse, error)
	// MultiInference API for multi-headed models.
	MultiInference(ctx context.Context, in *inference_go_proto.MultiInferenceRequest, opts ...grpc.CallOption) (*inference_go_proto.MultiInferenceResponse, error)
	// GetModelMetadata - provides access to metadata for loaded models.
	GetModelMetadata(ctx context.Context, in *get_model_metadata_go_proto.GetModelMetadataRequest, opts ...grpc.CallOption) (*get_model_metadata_go_proto.GetModelMetadataResponse, error)
}

type predictionServiceClient struct {
	cc *grpc.ClientConn
}

func NewPredictionServiceClient(cc *grpc.ClientConn) PredictionServiceClient {
	return &predictionServiceClient{cc}
}

func (c *predictionServiceClient) Classify(ctx context.Context, in *classification_go_proto.ClassificationRequest, opts ...grpc.CallOption) (*classification_go_proto.ClassificationResponse, error) {
	out := new(classification_go_proto.ClassificationResponse)
	err := c.cc.Invoke(ctx, "/tensorflow.serving.PredictionService/Classify", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *predictionServiceClient) Regress(ctx context.Context, in *regression_go_proto.RegressionRequest, opts ...grpc.CallOption) (*regression_go_proto.RegressionResponse, error) {
	out := new(regression_go_proto.RegressionResponse)
	err := c.cc.Invoke(ctx, "/tensorflow.serving.PredictionService/Regress", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *predictionServiceClient) Predict(ctx context.Context, in *predict_go_proto.PredictRequest, opts ...grpc.CallOption) (*predict_go_proto.PredictResponse, error) {
	out := new(predict_go_proto.PredictResponse)
	err := c.cc.Invoke(ctx, "/tensorflow.serving.PredictionService/Predict", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *predictionServiceClient) MultiInference(ctx context.Context, in *inference_go_proto.MultiInferenceRequest, opts ...grpc.CallOption) (*inference_go_proto.MultiInferenceResponse, error) {
	out := new(inference_go_proto.MultiInferenceResponse)
	err := c.cc.Invoke(ctx, "/tensorflow.serving.PredictionService/MultiInference", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

func (c *predictionServiceClient) GetModelMetadata(ctx context.Context, in *get_model_metadata_go_proto.GetModelMetadataRequest, opts ...grpc.CallOption) (*get_model_metadata_go_proto.GetModelMetadataResponse, error) {
	out := new(get_model_metadata_go_proto.GetModelMetadataResponse)
	err := c.cc.Invoke(ctx, "/tensorflow.serving.PredictionService/GetModelMetadata", in, out, opts...)
	if err != nil {
		return nil, err
	}
	return out, nil
}

// PredictionServiceServer is the server API for PredictionService service.
type PredictionServiceServer interface {
	// Classify.
	Classify(context.Context, *classification_go_proto.ClassificationRequest) (*classification_go_proto.ClassificationResponse, error)
	// Regress.
	Regress(context.Context, *regression_go_proto.RegressionRequest) (*regression_go_proto.RegressionResponse, error)
	// Predict -- provides access to loaded TensorFlow model.
	Predict(context.Context, *predict_go_proto.PredictRequest) (*predict_go_proto.PredictResponse, error)
	// MultiInference API for multi-headed models.
	MultiInference(context.Context, *inference_go_proto.MultiInferenceRequest) (*inference_go_proto.MultiInferenceResponse, error)
	// GetModelMetadata - provides access to metadata for loaded models.
	GetModelMetadata(context.Context, *get_model_metadata_go_proto.GetModelMetadataRequest) (*get_model_metadata_go_proto.GetModelMetadataResponse, error)
}

func RegisterPredictionServiceServer(s *grpc.Server, srv PredictionServiceServer) {
	s.RegisterService(&_PredictionService_serviceDesc, srv)
}

func _PredictionService_Classify_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(classification_go_proto.ClassificationRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(PredictionServiceServer).Classify(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/tensorflow.serving.PredictionService/Classify",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(PredictionServiceServer).Classify(ctx, req.(*classification_go_proto.ClassificationRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _PredictionService_Regress_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(regression_go_proto.RegressionRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(PredictionServiceServer).Regress(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/tensorflow.serving.PredictionService/Regress",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(PredictionServiceServer).Regress(ctx, req.(*regression_go_proto.RegressionRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _PredictionService_Predict_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(predict_go_proto.PredictRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(PredictionServiceServer).Predict(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/tensorflow.serving.PredictionService/Predict",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(PredictionServiceServer).Predict(ctx, req.(*predict_go_proto.PredictRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _PredictionService_MultiInference_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(inference_go_proto.MultiInferenceRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(PredictionServiceServer).MultiInference(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/tensorflow.serving.PredictionService/MultiInference",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(PredictionServiceServer).MultiInference(ctx, req.(*inference_go_proto.MultiInferenceRequest))
	}
	return interceptor(ctx, in, info, handler)
}

func _PredictionService_GetModelMetadata_Handler(srv interface{}, ctx context.Context, dec func(interface{}) error, interceptor grpc.UnaryServerInterceptor) (interface{}, error) {
	in := new(get_model_metadata_go_proto.GetModelMetadataRequest)
	if err := dec(in); err != nil {
		return nil, err
	}
	if interceptor == nil {
		return srv.(PredictionServiceServer).GetModelMetadata(ctx, in)
	}
	info := &grpc.UnaryServerInfo{
		Server:     srv,
		FullMethod: "/tensorflow.serving.PredictionService/GetModelMetadata",
	}
	handler := func(ctx context.Context, req interface{}) (interface{}, error) {
		return srv.(PredictionServiceServer).GetModelMetadata(ctx, req.(*get_model_metadata_go_proto.GetModelMetadataRequest))
	}
	return interceptor(ctx, in, info, handler)
}

var _PredictionService_serviceDesc = grpc.ServiceDesc{
	ServiceName: "tensorflow.serving.PredictionService",
	HandlerType: (*PredictionServiceServer)(nil),
	Methods: []grpc.MethodDesc{
		{
			MethodName: "Classify",
			Handler:    _PredictionService_Classify_Handler,
		},
		{
			MethodName: "Regress",
			Handler:    _PredictionService_Regress_Handler,
		},
		{
			MethodName: "Predict",
			Handler:    _PredictionService_Predict_Handler,
		},
		{
			MethodName: "MultiInference",
			Handler:    _PredictionService_MultiInference_Handler,
		},
		{
			MethodName: "GetModelMetadata",
			Handler:    _PredictionService_GetModelMetadata_Handler,
		},
	},
	Streams:  []grpc.StreamDesc{},
	Metadata: "third_party/tensorflow_serving/apis/prediction_service.proto",
}

// UnimplementedPredictionServiceServer implements PredictionServiceServer and returns Unimplemented for each method that is called.
//
// It allows users to override methods based on their name.
type UnimplementedPredictionServiceServer struct {
	handlers map[string]interface{}
}

func (u *UnimplementedPredictionServiceServer) SetMethodHandler(method string, handler interface{}) error {
	switch method {
	case "Classify":
		if h, ok := handler.(func(context.Context, *classification_go_proto.ClassificationRequest) (*classification_go_proto.ClassificationResponse, error)); ok {
			u.handlers[method] = h
		} else {
			return fmt.Errorf("Invalid handler type for method %q: %T, want %T", "Classify", handler, u.Classify)
		}
	case "Regress":
		if h, ok := handler.(func(context.Context, *regression_go_proto.RegressionRequest) (*regression_go_proto.RegressionResponse, error)); ok {
			u.handlers[method] = h
		} else {
			return fmt.Errorf("Invalid handler type for method %q: %T, want %T", "Regress", handler, u.Regress)
		}
	case "Predict":
		if h, ok := handler.(func(context.Context, *predict_go_proto.PredictRequest) (*predict_go_proto.PredictResponse, error)); ok {
			u.handlers[method] = h
		} else {
			return fmt.Errorf("Invalid handler type for method %q: %T, want %T", "Predict", handler, u.Predict)
		}
	case "MultiInference":
		if h, ok := handler.(func(context.Context, *inference_go_proto.MultiInferenceRequest) (*inference_go_proto.MultiInferenceResponse, error)); ok {
			u.handlers[method] = h
		} else {
			return fmt.Errorf("Invalid handler type for method %q: %T, want %T", "MultiInference", handler, u.MultiInference)
		}
	case "GetModelMetadata":
		if h, ok := handler.(func(context.Context, *get_model_metadata_go_proto.GetModelMetadataRequest) (*get_model_metadata_go_proto.GetModelMetadataResponse, error)); ok {
			u.handlers[method] = h
		} else {
			return fmt.Errorf("Invalid handler type for method %q: %T, want %T", "GetModelMetadata", handler, u.GetModelMetadata)
		}
	default:
		return fmt.Errorf("Unknown method name for service %q: %q", "PredictionService", method)
	}
	return nil
}

func (u *UnimplementedPredictionServiceServer) Classify(ctx context.Context, req *classification_go_proto.ClassificationRequest) (*classification_go_proto.ClassificationResponse, error) {
	if h := u.handlers["Classify"]; h != nil {
		return h.(func(context.Context, *classification_go_proto.ClassificationRequest) (*classification_go_proto.ClassificationResponse, error))(ctx, req)
	}
	return nil, grpc.Errorf(grpc.CodesUnimplemented, "method %q not implemented", "Classify")
}

func (u *UnimplementedPredictionServiceServer) Regress(ctx context.Context, req *regression_go_proto.RegressionRequest) (*regression_go_proto.RegressionResponse, error) {
	if h := u.handlers["Regress"]; h != nil {
		return h.(func(context.Context, *regression_go_proto.RegressionRequest) (*regression_go_proto.RegressionResponse, error))(ctx, req)
	}
	return nil, grpc.Errorf(grpc.CodesUnimplemented, "method %q not implemented", "Regress")
}

func (u *UnimplementedPredictionServiceServer) Predict(ctx context.Context, req *predict_go_proto.PredictRequest) (*predict_go_proto.PredictResponse, error) {
	if h := u.handlers["Predict"]; h != nil {
		return h.(func(context.Context, *predict_go_proto.PredictRequest) (*predict_go_proto.PredictResponse, error))(ctx, req)
	}
	return nil, grpc.Errorf(grpc.CodesUnimplemented, "method %q not implemented", "Predict")
}

func (u *UnimplementedPredictionServiceServer) MultiInference(ctx context.Context, req *inference_go_proto.MultiInferenceRequest) (*inference_go_proto.MultiInferenceResponse, error) {
	if h := u.handlers["MultiInference"]; h != nil {
		return h.(func(context.Context, *inference_go_proto.MultiInferenceRequest) (*inference_go_proto.MultiInferenceResponse, error))(ctx, req)
	}
	return nil, grpc.Errorf(grpc.CodesUnimplemented, "method %q not implemented", "MultiInference")
}

func (u *UnimplementedPredictionServiceServer) GetModelMetadata(ctx context.Context, req *get_model_metadata_go_proto.GetModelMetadataRequest) (*get_model_metadata_go_proto.GetModelMetadataResponse, error) {
	if h := u.handlers["GetModelMetadata"]; h != nil {
		return h.(func(context.Context, *get_model_metadata_go_proto.GetModelMetadataRequest) (*get_model_metadata_go_proto.GetModelMetadataResponse, error))(ctx, req)
	}
	return nil, grpc.Errorf(grpc.CodesUnimplemented, "method %q not implemented", "GetModelMetadata")
}

func NewUnimplementedPredictionServiceServer() grpc.UnimplementedService {
	return &UnimplementedPredictionServiceServer{handlers: make(map[string]interface{})}
}

func registerUnimplementedPredictionServiceServer(s *grpc.Server, srv grpc.UnimplementedService) {
	s.RegisterService(&_PredictionService_serviceDesc, srv)
}

func init() {
	grpc.InitUnimplementedServiceFuncs("tensorflow.serving.PredictionService", NewUnimplementedPredictionServiceServer, registerUnimplementedPredictionServiceServer)
}
